

# My Notes on trying to setup and use Insights/Analytics


## Background:

### Credentials


Credentials for the LMS RDSMS is stored here:

`/edx/etc/edx-analytics-pipeline/input.json` 


With Json like:

`{"username": "read_only", "host": "my_rds_mysql_path", "password": "some-password", "port": 3306}`
    

Credentials for storing results in local mysql are stored here:

`/edx/etc/edx-analytics-pipeline/output.json` 

With Json like:

`{"username": "pipeline001", "host": "localhost", "password": "some-password", "port": 3306}`


## Generate Enrollment Events
This task produces one file of all enrollments:

`
remote-task CourseEnrollmentEventsTask \
--host localhost \
--user ubuntu \
--remote-name analyticstack \
--skip-setup \
--wait \
--local-scheduler \
--verbose \
--interval $(date +%Y-%m-%d -d "04/01/2017")-$(date +%Y-%m-%d -d "06/09/2017") \
--local-scheduler \
--n-reduce-tasks 1
`

If it finishes successfully, the end result is something like: /edx-analytics-pipeline/marker/130580543084933421-temp-2017-06-09T01-20-54.896365


## Doing something with Enrollment Events

Then I run this task to do something with those events. This task assumes that the folder /edx-analytics-pipeline/warehouse/course_enrollment_events
has a bunch of events already in it. These should have been generated by the CourseEnrollmentEventsTask above.

To check:

`hdfs dfs -ls /edx-analytics-pipeline/warehouse/course_enrollment_events`

To run this command:


remote-task ImportEnrollmentsIntoMysql \
--overwrite-n-days 3 \
--override-config /home/ubuntu/override.cfg
--host localhost \
--user ubuntu \
--remote-name analyticstack \
--skip-setup \
--wait \
--local-scheduler  \
--verbose \
--interval 2017 \
--local-scheduler



remote-task AnswerDistributionWorkflow \
		--host localhost \
		--user ubuntu \
		--remote-name analyticstack \
		--skip-setup \
		--wait \
  		--local-scheduler  \
  		--verbose \
  		--src hdfs://localhost:9000/data \
  		--dest hdfs://localhost:9000/tmp/pipeline-task-scheduler/AnswerDistributionWorkflow/1449177792/dest \
  		--name pt_1449177792 \
  		--output-root hdfs://localhost:9000/tmp/pipeline-task-scheduler/AnswerDistributionWorkflow/1449177792/course \
  		--include "*tracking.log*.gz" \
  		--manifest hdfs://localhost:9000/tmp/pipeline-task-scheduler/AnswerDistributionWorkflow/1449177792/manifest.txt \
  		--base-input-format "org.edx.hadoop.input.ManifestTextInputFormat"  \
  		--lib-jar hdfs://localhost:9000/edx-analytics-pipeline/site-packages/edx-analytics-hadoop-util.jar  \
  		--n-reduce-tasks 1 \
  		--marker hdfs://localhost:9000/tmp/pipeline-task-scheduler/AnswerDistributionWorkflow/1449177792/marker  \
  		--credentials /edx/etc/edx-analytics-pipeline/output.json